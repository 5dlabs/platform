name: Unified Platform CI/CD

on:
  push:
    branches: [ main ]
    tags: [ 'v*' ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  REGISTRY: ghcr.io
  IMAGE_BASE: ${{ github.repository }}

jobs:
  # Change detection job
  changes:
    runs-on: ubuntu-latest
    outputs:
      claude-code: ${{ steps.filter.outputs.claude-code }}
      orchestrator: ${{ steps.filter.outputs.orchestrator }}
      taskrun: ${{ steps.filter.outputs.taskrun }}
      infra: ${{ steps.filter.outputs.infra }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v2
        id: filter
        with:
          filters: |
            claude-code:
              - 'Dockerfile'
              - 'package*.json'
              - 'src/**'
              - 'scripts/**'
            orchestrator:
              - 'orchestrator/**'
              - '!orchestrator/**/*.md'
            taskrun:
              - 'orchestrator/orchestrator-core/src/controllers/taskrun.rs'
              - 'orchestrator/orchestrator-core/src/crds/**'
              - 'infra/crds/**'
            infra:
              - 'infra/**'
              - '.github/workflows/**'

  # Version determination
  version:
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
      is-release: ${{ steps.version.outputs.is-release }}
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Determine version
        id: version
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            VERSION="${{ github.ref_name }}"
            echo "version=${VERSION}" >> $GITHUB_OUTPUT
            echo "is-release=true" >> $GITHUB_OUTPUT
          else
            # Use commit SHA for non-release builds
            VERSION="main-$(git rev-parse --short HEAD)"
            echo "version=${VERSION}" >> $GITHUB_OUTPUT
            echo "is-release=false" >> $GITHUB_OUTPUT
          fi

  # Parallel linting and testing
  lint-rust:
    needs: changes
    if: needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || github.event_name == 'push'
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy
      - uses: Swatinem/rust-cache@v2
        with:
          workspaces: orchestrator -> target
          # Share cache between jobs
          shared-key: "rust-cache-agent-platform"
      - name: Check formatting
        working-directory: ./orchestrator
        run: cargo fmt --all -- --check
      - name: Run Clippy
        working-directory: ./orchestrator
        run: cargo clippy --all-targets --all-features -- -D warnings

  test-rust:
    needs: changes
    if: needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || github.event_name == 'push'
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
        with:
          workspaces: orchestrator -> target
          # Share cache between jobs
          shared-key: "rust-cache-agent-platform"
      - name: Run tests
        working-directory: ./orchestrator
        run: cargo test --all-features --all-targets

  # Integration tests
  integration-tests:
    needs: [lint-rust, test-rust]
    if: always() && !cancelled() && (needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || needs.changes.outputs.infra == 'true' || github.event_name == 'push')
    runs-on: ubuntu-22.04
    steps:
      - uses: actions/checkout@v4

      # Cache kubectl binary
      - name: Cache kubectl
        uses: actions/cache@v4
        with:
          path: /usr/local/bin/kubectl
          key: kubectl-agent-platform-${{ runner.os }}-1.30.0

      - name: Create Kind cluster
        uses: helm/kind-action@v1.8.0
        with:
          cluster_name: platform-test
          config: .github/kind-config.yaml
          kubectl_version: v1.30.0
          # Wait longer for cluster to be ready
          wait: 120s
          verbosity: 1

      - name: Install CRDs
        run: |
          echo "Installing TaskRun CRD..."
          kubectl apply -f infra/crds/taskrun-crd.yaml
          echo "Waiting for CRD to be established..."
          kubectl wait --for condition=established --timeout=60s crd/taskruns.orchestrator.io
          echo "CRD installed successfully"

      - name: Test Helm Chart Structure
        run: |
          echo "üéØ Testing orchestrator Helm chart..."

          # Install Helm
          curl -fsSL https://get.helm.sh/helm-v3.14.0-linux-amd64.tar.gz | tar xz
          sudo mv linux-amd64/helm /usr/local/bin/
          helm version

          # Lint the Helm chart
          helm lint ./infra/charts/orchestrator

          # Test template generation with different values
          echo "üìã Testing default values..."
          helm template orchestrator ./infra/charts/orchestrator > /tmp/default.yaml
          kubectl --dry-run=client apply -f /tmp/default.yaml

          echo "üìã Testing with secrets..."
          helm template orchestrator ./infra/charts/orchestrator \
            --set secrets.anthropicApiKey="test-key" \
            --set secrets.githubToken="test-token" > /tmp/with-secrets.yaml
          kubectl --dry-run=client apply -f /tmp/with-secrets.yaml

          echo "üìã Testing with custom image..."
          helm template orchestrator ./infra/charts/orchestrator \
            --set image.repository="custom/repo" \
            --set image.tag="v1.2.3" > /tmp/custom-image.yaml
          kubectl --dry-run=client apply -f /tmp/custom-image.yaml

          echo "‚úÖ Helm chart validation passed!"

      - name: Deploy test resources
        run: |
          # Create test namespace
          kubectl create namespace test-platform

          # Apply test TaskRun
          kubectl apply -f infra/crds/test-taskrun.yaml -n test-platform || true

      - name: Verify CRD deployment
        run: |
          kubectl get crd taskruns.orchestrator.io
          kubectl get taskruns -A

  # Security scanning (non-blocking, self-hosted)
  security-scan:
    needs: changes
    if: github.event_name == 'push'  # Only on main branch
    runs-on: ubuntu-latest
    continue-on-error: true  # Non-blocking
    steps:
      - uses: actions/checkout@v4

      # Cache Trivy DB
      - name: Cache Trivy database
        uses: actions/cache@v4
        with:
          path: ~/.cache/trivy
          key: trivy-db-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            trivy-db-${{ runner.os }}-

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          cache-dir: ~/.cache/trivy

      - name: Upload Trivy scan results
        uses: actions/upload-artifact@v4
        with:
          name: trivy-results
          path: trivy-results.sarif

      # Cache cargo-audit binary
      - name: Cache cargo-audit
        uses: actions/cache@v4
        id: cargo-audit-cache
        with:
          path: ~/.cargo/bin/cargo-audit
          key: cargo-audit-${{ runner.os }}-0.21.2

      - name: Install cargo-audit
        if: steps.cargo-audit-cache.outputs.cache-hit != 'true'
        run: cargo install cargo-audit --version 0.21.2

      - name: Rust security audit
        working-directory: ./orchestrator
        run: cargo audit || true  # Non-blocking

  # Test coverage reporting (non-blocking)
  test-coverage:
    needs: changes
    if: needs.changes.outputs.orchestrator == 'true' || needs.changes.outputs.taskrun == 'true' || github.event_name == 'push'
    runs-on: ubuntu-22.04
    continue-on-error: true  # Non-blocking
    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
        with:
          components: llvm-tools-preview
      - uses: Swatinem/rust-cache@v2
        with:
          workspaces: orchestrator -> target
          # Share cache between jobs
          shared-key: "rust-cache-agent-platform"

      # Install cargo-llvm-cov for coverage
      - name: Cache cargo-llvm-cov
        uses: actions/cache@v4
        id: cargo-llvm-cov-cache
        with:
          path: ~/.cargo/bin/cargo-llvm-cov
          key: cargo-llvm-cov-${{ runner.os }}-0.6.8

      - name: Install cargo-llvm-cov
        if: steps.cargo-llvm-cov-cache.outputs.cache-hit != 'true'
        run: cargo install cargo-llvm-cov --version 0.6.8

      - name: Generate test coverage
        working-directory: ./orchestrator
        run: |
          echo "üìä Generating test coverage report..."
          cargo llvm-cov --all-features --workspace --lcov --output-path lcov.info || true
          cargo llvm-cov --all-features --workspace --html --output-dir coverage-html || true

          # Generate summary
          echo "## Test Coverage Summary" > coverage-summary.md
          echo "" >> coverage-summary.md
          if [ -f "lcov.info" ]; then
            # Extract coverage percentage from lcov file
            COVERAGE=$(grep -E "^LF:|^LH:" lcov.info | awk -F: '{if($1=="LF") lf+=$2; if($1=="LH") lh+=$2} END {if(lf>0) printf "%.1f", (lh/lf)*100; else print "0.0"}')
            echo "üìä **Overall Coverage**: ${COVERAGE}%" >> coverage-summary.md
            echo "Coverage: ${COVERAGE}%"
          else
            echo "‚ö†Ô∏è **Coverage**: Could not generate coverage report" >> coverage-summary.md
            echo "Coverage report generation failed"
          fi

          echo "" >> coverage-summary.md
          echo "üìÅ Detailed HTML report available in artifacts" >> coverage-summary.md

      - name: Upload coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: |
            orchestrator/lcov.info
            orchestrator/coverage-html/
            orchestrator/coverage-summary.md

      - name: Display coverage summary
        working-directory: ./orchestrator
        run: |
          if [ -f "coverage-summary.md" ]; then
            echo "üìä Test Coverage Results:"
            cat coverage-summary.md
          else
            echo "‚ö†Ô∏è Coverage summary not available"
          fi

  # Build images
  build-claude-code:
    needs: [version, lint-rust, test-rust, integration-tests]
    if: always() && !cancelled() && (needs.changes.outputs.claude-code == 'true' || github.event_name == 'push')
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4

      - name: Get Claude Code CLI version
        id: claude-version
        run: |
          CLAUDE_VERSION=$(npm view @anthropic-ai/claude-code version)
          echo "CLAUDE_VERSION=$CLAUDE_VERSION" >> $GITHUB_OUTPUT
          echo "üì¶ Latest Claude Code CLI version on npm: $CLAUDE_VERSION"

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Check if Claude Code image exists
        id: claude-exists
        run: |
          echo "üîç Checking if we have built Claude Code version ${{ steps.claude-version.outputs.CLAUDE_VERSION }}"
          echo "Looking for: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }}"
          if docker manifest inspect ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }} > /dev/null 2>&1; then
            echo "‚úÖ We already have this version built, skipping build"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "üèóÔ∏è We haven't built this version yet, will build"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Set up Docker Buildx
        if: steps.claude-exists.outputs.exists == 'false'
        uses: docker/setup-buildx-action@v3

      - name: Build and push Claude Code image
        if: steps.claude-exists.outputs.exists == 'false'
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          platforms: linux/amd64,linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            BUILDKIT_INLINE_CACHE=1
            CLAUDE_CODE_VERSION=${{ steps.claude-version.outputs.CLAUDE_VERSION }}

      - name: Tag existing image with platform version
        if: steps.claude-exists.outputs.exists == 'true' && github.event_name != 'pull_request'
        run: |
          echo "üè∑Ô∏è Tagging existing image with platform version"
          docker buildx imagetools create \
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ steps.claude-version.outputs.CLAUDE_VERSION }} \
            --tag ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}

  build-orchestrator:
    needs: [version, lint-rust, test-rust, integration-tests]
    if: always() && !cancelled() && (needs.changes.outputs.orchestrator == 'true' || github.event_name == 'push')
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write
    steps:
      - uses: actions/checkout@v4

      - uses: dtolnay/rust-toolchain@stable
      - uses: Swatinem/rust-cache@v2
        with:
          workspaces: orchestrator -> target
          # Share cache with other jobs and cache release artifacts
          shared-key: "rust-cache"
          cache-targets: true

      - name: Build release binary
        working-directory: ./orchestrator
        run: |
          cargo build --release --package orchestrator-core
          cp target/release/orchestrator-core orchestrator

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Orchestrator image
        uses: docker/build-push-action@v5
        with:
          context: ./orchestrator
          file: ./orchestrator/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:${{ needs.version.outputs.version }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          # Add registry cache for better layer reuse
          build-args: |
            BUILDKIT_INLINE_CACHE=1

  build-gemini-cli:
    needs: [version, lint-rust, test-rust, integration-tests]
    if: always() && !cancelled() && github.event_name == 'push'  # Always build Gemini CLI on push
    runs-on: ubuntu-22.04
    permissions:
      contents: read
      packages: write
    steps:
      - name: Checkout platform repository
        uses: actions/checkout@v4

      - name: Checkout Gemini CLI repository
        uses: actions/checkout@v4
        with:
          repository: google-gemini/gemini-cli
          ref: main
          path: gemini-cli

      - uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: gemini-cli/package-lock.json

      - name: Install dependencies and build packages
        working-directory: ./gemini-cli
        run: |
          npm ci
          npm run build:packages
          npm pack -w @google/gemini-cli --pack-destination ./packages/cli/dist
          npm pack -w @google/gemini-cli-core --pack-destination ./packages/core/dist

      - name: Get CLI version
        id: cli-version
        working-directory: ./gemini-cli
        run: |
          CLI_VERSION=$(node -p "require('./packages/cli/package.json').version")
          echo "CLI_VERSION=$CLI_VERSION" >> $GITHUB_OUTPUT
          echo "üì¶ Gemini CLI version from source: $CLI_VERSION"

      - name: Log in to Container Registry
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Check if Gemini CLI image exists
        id: gemini-exists
        run: |
          echo "üîç Checking if we have built Gemini CLI version ${{ steps.cli-version.outputs.CLI_VERSION }}"
          echo "Looking for: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }}"
          if docker manifest inspect ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }} > /dev/null 2>&1; then
            echo "‚úÖ We already have this version built, skipping build"
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "üèóÔ∏è We haven't built this version yet, will build"
            echo "exists=false" >> $GITHUB_OUTPUT
          fi

      - name: Set up Docker Buildx
        if: steps.gemini-exists.outputs.exists == 'false'
        uses: docker/setup-buildx-action@v3

      - name: Build and push Gemini CLI image
        if: steps.gemini-exists.outputs.exists == 'false'
        uses: docker/build-push-action@v5
        with:
          context: ./gemini-cli
          file: ./gemini-cli/Dockerfile
          platforms: linux/amd64,linux/arm64
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:latest
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          # Add registry cache for better layer reuse
          build-args: |
            BUILDKIT_INLINE_CACHE=1
            CLI_VERSION_ARG=${{ steps.cli-version.outputs.CLI_VERSION }}

      - name: Tag existing image with platform version
        if: steps.gemini-exists.outputs.exists == 'true' && github.event_name != 'pull_request'
        run: |
          echo "üè∑Ô∏è Tagging existing image with platform version"
          docker buildx imagetools create \
            ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ steps.cli-version.outputs.CLI_VERSION }} \
            --tag ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}

  # Deploy to single environment (only on main branch)
  deploy:
    needs: [version, build-claude-code, build-orchestrator, build-gemini-cli]
    if: (github.ref == 'refs/heads/main' || startsWith(github.ref, 'refs/tags/v')) && github.event_name == 'push'
    runs-on: [self-hosted, agent-platform]
    steps:
      - uses: actions/checkout@v4

      - name: Setup tools
        run: |
          # Create local bin directory
          mkdir -p $HOME/bin
          
          # Copy tools from shared location if they exist
          if [ -f /shared/kubectl ]; then
            cp /shared/kubectl $HOME/bin/
            chmod +x $HOME/bin/kubectl
          fi
          
          if [ -f /shared/helm ]; then
            cp /shared/helm $HOME/bin/
            chmod +x $HOME/bin/helm
          fi
          
          # Add to PATH for this job
          echo "$HOME/bin" >> $GITHUB_PATH

      - name: Verify tools
        run: |
          echo "Checking tools..."
          kubectl version --client || echo "kubectl failed"
          helm version || echo "helm failed"

      - name: Setup kubeconfig
        run: |
          mkdir -p ~/.kube
          
          # Try to decode as base64 first, if that fails, use as-is
          if echo "${{ secrets.KUBECONFIG }}" | base64 -d > ~/.kube/config.tmp 2>/dev/null; then
            # Check if decoded content is valid YAML/JSON
            if grep -q "apiVersion\|kind" ~/.kube/config.tmp; then
              echo "‚úÖ Using base64-decoded kubeconfig"
              mv ~/.kube/config.tmp ~/.kube/config
            else
              echo "‚úÖ Using kubeconfig as-is (not base64)"
              echo "${{ secrets.KUBECONFIG }}" > ~/.kube/config
            fi
          else
            echo "‚úÖ Using kubeconfig as-is (not base64)"
            echo "${{ secrets.KUBECONFIG }}" > ~/.kube/config
          fi
          
          rm -f ~/.kube/config.tmp
          chmod 600 ~/.kube/config
          
          # Debug: Show first few lines (without sensitive data)
          echo "Kubeconfig structure:"
          grep -E "^(apiVersion|kind|clusters|contexts|users):" ~/.kube/config | head -5

          # Verify cluster connection
          kubectl cluster-info
          kubectl get nodes

      - name: Install TaskRun CRD
        run: |
          echo "üìã Installing TaskRun CRD..."
          kubectl apply -f infra/crds/taskrun-crd.yaml

          # Wait for CRD to be established
          kubectl wait --for condition=established --timeout=60s crd/taskruns.orchestrator.io
          echo "‚úÖ TaskRun CRD installed successfully"

      - name: Deploy Orchestrator with Helm
        run: |
          echo "üöÄ Deploying platform version ${{ needs.version.outputs.version }}"

          # Deploy the orchestrator (TaskRun controller manages Claude Code and Gemini CLI agents)
          helm upgrade --install orchestrator ./infra/charts/orchestrator \
            --namespace orchestrator \
            --create-namespace \
            --set image.repository=${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator \
            --set image.tag=${{ needs.version.outputs.version }} \
            --set secrets.anthropicApiKey="${{ secrets.ANTHROPIC_API_KEY }}" \
            --set secrets.githubToken="${{ secrets.GH_TOKEN_FOR_AGENTS }}" \
            --timeout 10m \
            --wait \
            --atomic

          # Apply the controller configuration
          echo "üìã Applying TaskRun controller configuration..."
          kubectl apply -f infra/crds/taskrun-controller-config.yaml

          echo "üìã Available agent images for TaskRun resources:"
          echo "- Claude Code: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}"
          echo "- Gemini CLI: ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}"

      - name: Verify deployment
        run: |
          echo "üîç Verifying deployment..."
          kubectl get pods -n orchestrator
          kubectl get services -n orchestrator
          kubectl get taskruns -A || echo "No TaskRuns found (expected for fresh deployment)"

          # Wait for orchestrator to be ready
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=orchestrator -n orchestrator --timeout=300s

          echo "‚úÖ Deployment verification complete!"

      - name: Deployment notification
        run: |
          echo "üéâ Successfully deployed platform version ${{ needs.version.outputs.version }}"
          echo ""
          echo "Images deployed:"
          echo "- ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}"
          echo "- ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:${{ needs.version.outputs.version }}"
          echo "- ${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}"
          echo "  (Gemini CLI also tagged with its official version from Google)"
          echo ""
          echo "Cluster status:"
          kubectl get pods -n orchestrator -o wide
          echo ""
          echo "üåê Orchestrator should be accessible through your Twingate network"

  # Create release (only for tags)
  release:
    needs: [version, deploy]
    if: needs.version.outputs.is-release == 'true'
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4

      - name: Create Release
        uses: softprops/action-gh-release@v1
        with:
          tag_name: ${{ needs.version.outputs.version }}
          name: Release ${{ needs.version.outputs.version }}
          body: |
            ## Container Images

            This release includes the following container images:
            - `${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/claude-code:${{ needs.version.outputs.version }}`
            - `${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/orchestrator:${{ needs.version.outputs.version }}`
            - `${{ env.REGISTRY }}/${{ env.IMAGE_BASE }}/gemini-cli:${{ needs.version.outputs.version }}`

            ## Deployment

            Update your Helm values or Kubernetes manifests to use the new image tags.
          draft: false
          prerelease: false